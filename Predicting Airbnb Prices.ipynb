{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets go into the business case study observation. We want to overcome to the the problem that the hosts face in determining the optimal night rental price. The price should be nearby to the marketplace or renters. If the charge > market place, then renter will go to other alternaitve and if low then we will miss out with the potential revenue. So, the strategy would be:\n",
    "1) We will find few listings that are similar to our\n",
    "2) We will average those listings\n",
    "3) Set this listing price to the calculated average price\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build machine learning model to automate this process with the technique called k-nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3723 entries, 574 to 1061\n",
      "Data columns (total 19 columns):\n",
      "host_response_rate      3289 non-null object\n",
      "host_acceptance_rate    3109 non-null object\n",
      "host_listings_count     3723 non-null int64\n",
      "accommodates            3723 non-null int64\n",
      "room_type               3723 non-null object\n",
      "bedrooms                3702 non-null float64\n",
      "bathrooms               3696 non-null float64\n",
      "beds                    3712 non-null float64\n",
      "price                   3723 non-null float64\n",
      "cleaning_fee            2335 non-null object\n",
      "security_deposit        1426 non-null object\n",
      "minimum_nights          3723 non-null int64\n",
      "maximum_nights          3723 non-null int64\n",
      "number_of_reviews       3723 non-null int64\n",
      "latitude                3723 non-null float64\n",
      "longitude               3723 non-null float64\n",
      "city                    3723 non-null object\n",
      "zipcode                 3714 non-null object\n",
      "state                   3723 non-null object\n",
      "dtypes: float64(6), int64(5), object(8)\n",
      "memory usage: 581.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "dc_listings=pd.read_csv(\"dc_airbnb.csv\")\n",
    "dc_listings=dc_listings.loc[np.random.permutation(len(dc_listings))]\n",
    "stripped_commas = dc_listings['price'].str.replace(',', '')\n",
    "stripped_dollars = stripped_commas.str.replace('$', '')\n",
    "dc_listings['price'] = stripped_dollars.astype('float')\n",
    "print(dc_listings.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following columns contain non-numerical values:\n",
    "room_type: e.g. Private room\n",
    "city: e.g. Washington\n",
    "state: e.g. DC\n",
    "\n",
    "while these columns contain numerical but non-ordinal values:\n",
    "latitude: e.g. 38.913458\n",
    "longitude: e.g. -77.031\n",
    "zipcode: e.g. 20009\n",
    "\n",
    "While we could convert the host_response_rate and host_acceptance_rate columns to be numerical (right now they're object data types and contain the % sign), these columns describe the host and not the living space itself. Since a host could have many living spaces and we don't have enough information to uniquely group living spaces to the hosts themselves, let's avoid using any columns that don't directly describe the living space or the listing itself:\n",
    "\n",
    "host_response_rate\n",
    "host_acceptance_rate\n",
    "host_listings_count\n",
    "\n",
    "Removing:\n",
    "-3 containing non-numerical values\n",
    "-3 containing numerical but non-ordinal values\n",
    "-3 describing the host instead of the living space itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accommodates            0\n",
      "bedrooms               21\n",
      "bathrooms              27\n",
      "beds                   11\n",
      "price                   0\n",
      "cleaning_fee         1388\n",
      "security_deposit     2297\n",
      "minimum_nights          0\n",
      "maximum_nights          0\n",
      "number_of_reviews       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "drop_columns=['room_type', 'city', 'state', 'latitude', 'longitude', 'zipcode', 'host_response_rate', 'host_acceptance_rate', 'host_listings_count']\n",
    "dc_listings=dc_listings.drop(drop_columns,axis=1)\n",
    "print(dc_listings.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the remaining columns, 3 columns have a few missing values (less than 1% of the total number of rows):\n",
    "-bedrooms\n",
    "-bathrooms\n",
    "-beds\n",
    "Since the number of rows containing missing values for one of these 3 columns is low, we can select and remove those rows without losing much information. There are also 2 columns that have a large number of missing values:\n",
    "\n",
    "cleaning_fee - 37.3% of the rows\n",
    "security_deposit - 61.7% of the rows\n",
    "and we can't handle these easily. We can't just remove the rows containing missing values for these 2 columns because we'd miss out on the majority of the observations in the dataset. Instead, let's remove these 2 columns entirely from consideration\n",
    "\n",
    "1)Drop the cleaning_fee and security_deposit columns from dc_listings.\n",
    "2)Then, remove all rows that contain a missing value for the bedrooms, bathrooms, or beds column from dc_listings.\n",
    "3)You can accomplish this by using the Dataframe method dropna() and setting the axis parameter to 0.\n",
    "Since only the bedrooms, bathrooms, and beds columns contain any missing values, rows containing missing values in these columns will be removed.\n",
    "4)Display the null value counts for the updated dc_listings Dataframe to confirm that there are no missing values left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accommodates         0\n",
      "bedrooms             0\n",
      "bathrooms            0\n",
      "beds                 0\n",
      "price                0\n",
      "minimum_nights       0\n",
      "maximum_nights       0\n",
      "number_of_reviews    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "drop_columns1=['cleaning_fee','security_deposit']\n",
    "dc_listings=dc_listings.drop(drop_columns1,axis=1)\n",
    "dc_listings=dc_listings.dropna(axis=0)\n",
    "print(dc_listings.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3671 entries, 574 to 1061\n",
      "Data columns (total 8 columns):\n",
      "accommodates         3671 non-null int64\n",
      "bedrooms             3671 non-null float64\n",
      "bathrooms            3671 non-null float64\n",
      "beds                 3671 non-null float64\n",
      "price                3671 non-null float64\n",
      "minimum_nights       3671 non-null int64\n",
      "maximum_nights       3671 non-null int64\n",
      "number_of_reviews    3671 non-null int64\n",
      "dtypes: float64(4), int64(4)\n",
      "memory usage: 258.1 KB\n",
      "      accommodates  bedrooms  bathrooms  beds  price  minimum_nights  \\\n",
      "574              2       1.0        1.0   1.0  125.0               1   \n",
      "1593             2       1.0        1.5   1.0   85.0               1   \n",
      "3091             1       1.0        0.5   1.0   50.0               1   \n",
      "420              2       1.0        1.0   1.0  209.0               4   \n",
      "808             12       5.0        2.0   5.0  215.0               2   \n",
      "3492             8       4.0        2.5   5.0  350.0               4   \n",
      "364              3       0.0        1.0   2.0  115.0               2   \n",
      "1412             2       1.0        1.0   1.0  110.0               2   \n",
      "3219             3       0.0        1.0   1.0   99.0               2   \n",
      "756              2       1.0        1.0   1.0   49.0               1   \n",
      "3089             4       1.0        1.0   3.0  100.0               2   \n",
      "1724             8       3.0        2.0   4.0  395.0               3   \n",
      "3358             2       0.0        1.0   1.0   70.0               1   \n",
      "1131             2       0.0        1.0   1.0   99.0               1   \n",
      "169              1       1.0        1.0   1.0   55.0               1   \n",
      "968              2       1.0        1.0   1.0   74.0               1   \n",
      "2430             2       1.0        1.0   1.0   80.0               4   \n",
      "2418             1       1.0        1.0   1.0   55.0               1   \n",
      "1555             2       1.0        1.0   1.0   42.0               1   \n",
      "3045             5       1.0        1.0   1.0  120.0               2   \n",
      "102              4       2.0        2.0   2.0  299.0               3   \n",
      "1992             4       1.0        1.0   2.0  100.0               1   \n",
      "99               4       2.0        2.0   3.0  180.0               7   \n",
      "2914             5       2.0        2.0   2.0  258.0               3   \n",
      "1729             3       1.0        1.0   2.0   91.0               4   \n",
      "1241             1       1.0        1.0   1.0   75.0              14   \n",
      "2169             2       1.0        1.0   1.0   95.0               1   \n",
      "829              2       0.0        1.0   1.0  178.0               1   \n",
      "1396             2       0.0        1.0   1.0  200.0               2   \n",
      "3081             1       1.0        1.0   1.0   31.0               1   \n",
      "...            ...       ...        ...   ...    ...             ...   \n",
      "3049             2       1.0        1.0   1.0  139.0               1   \n",
      "3287             4       1.0        1.0   3.0   68.0               1   \n",
      "2628             2       1.0        1.0   1.0   99.0               1   \n",
      "562             14       5.0        3.0   7.0  599.0               1   \n",
      "2446             4       2.0        2.0   2.0  199.0               1   \n",
      "668              5       1.0        1.0   2.0  120.0               1   \n",
      "3562             4       1.0        2.0   2.0   90.0               1   \n",
      "252              1       1.0        1.0   1.0  135.0               1   \n",
      "2955             2       1.0        1.0   1.0   82.0               1   \n",
      "2516             6       2.0        1.0   5.0  149.0               1   \n",
      "2962             2       1.0        1.0   1.0  180.0               3   \n",
      "357              3       1.0        1.0   1.0  189.0               5   \n",
      "1278             2       1.0        0.0   1.0   85.0               1   \n",
      "1300             2       1.0        1.0   1.0   75.0               1   \n",
      "1202             2       0.0        1.0   1.0   85.0               1   \n",
      "3353             2       1.0        1.0   1.0  150.0               1   \n",
      "3462             4       1.0        1.0   2.0  115.0               3   \n",
      "2556             4       2.0        1.5   2.0  150.0               3   \n",
      "2797             6       2.0        1.0   2.0  165.0               3   \n",
      "3655             2       1.0        1.0   1.0   90.0               1   \n",
      "129              3       1.0        1.0   2.0  246.0               3   \n",
      "144              2       1.0        1.0   1.0  120.0               1   \n",
      "960              2       1.0        1.0   1.0  165.0               1   \n",
      "2895             6       2.0        1.0   3.0  150.0               1   \n",
      "3717             7       3.0        2.0   3.0  285.0               2   \n",
      "2763             1       0.0        1.0   1.0   75.0               4   \n",
      "905              1       0.0        1.0   1.0   95.0               5   \n",
      "1096             2       1.0        0.0   1.0  100.0               1   \n",
      "235              8       2.0        1.0   4.0  194.0               1   \n",
      "1061             6       1.0        2.5   6.0   36.0               2   \n",
      "\n",
      "      maximum_nights  number_of_reviews  \n",
      "574                4                149  \n",
      "1593              30                 49  \n",
      "3091            1125                  1  \n",
      "420              730                  2  \n",
      "808             1825                 34  \n",
      "3492            1125                  1  \n",
      "364             1125                 63  \n",
      "1412            1125                  5  \n",
      "3219              14                 45  \n",
      "756             1125                  3  \n",
      "3089            1125                  1  \n",
      "1724            1125                  0  \n",
      "3358            1125                  2  \n",
      "1131            1125                  0  \n",
      "169               14                  1  \n",
      "968             1125                  2  \n",
      "2430              10                 19  \n",
      "2418              60                  7  \n",
      "1555            1125                 17  \n",
      "3045            1125                 92  \n",
      "102              730                  7  \n",
      "1992            1125                  2  \n",
      "99                21                 48  \n",
      "2914            1125                  0  \n",
      "1729            1125                  0  \n",
      "1241             180                 12  \n",
      "2169            1125                 19  \n",
      "829             1125                  4  \n",
      "1396              90                  0  \n",
      "3081               6                  3  \n",
      "...              ...                ...  \n",
      "3049            1125                 14  \n",
      "3287            1125                135  \n",
      "2628              30                 61  \n",
      "562             1125                  0  \n",
      "2446              15                  0  \n",
      "668             1125                  2  \n",
      "3562            1125                  2  \n",
      "252             1125                  0  \n",
      "2955            1125                 25  \n",
      "2516             365                 32  \n",
      "2962            1125                  1  \n",
      "357              730                  0  \n",
      "1278              60                206  \n",
      "1300               8                  2  \n",
      "1202              60                 30  \n",
      "3353            1125                  0  \n",
      "3462              60                 63  \n",
      "2556            1125                  1  \n",
      "2797              30                  0  \n",
      "3655            1125                 14  \n",
      "129             1125                  0  \n",
      "144             1125                  0  \n",
      "960               29                 19  \n",
      "2895            1125                  1  \n",
      "3717            1125                  8  \n",
      "2763              20                  1  \n",
      "905             1125                  0  \n",
      "1096            1125                 15  \n",
      "235             1125                  8  \n",
      "1061              30                  7  \n",
      "\n",
      "[3671 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "dc_listings.info()\n",
    "print(dc_listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what we noticed that while the accommodates, bedrooms, bathrooms, beds, and minimum_nights columns hover between 0 and 12 (at least in the first few rows), the values in the maximum_nights and number_of_reviews columns span much larger ranges. For example, the maximum_nights column has values as low as 4 and high as 1825, in the first few rows itself. If we use these 2 columns as part of a k-nearest neighbors model, these attributes could end up having an outsized effect on the distance calculations because of the largeness of the values.\n",
    "\n",
    "For example, 2 living spaces could be identical across every attribute but be vastly different just on the maximum_nights column. If one listing had a maximum_nights value of 1825 and the other a maximum_nights value of 4, because of the way Euclidean distance is calculated, these listings would be considered very far apart because of the outsized effect the largeness of the values had on the overall Euclidean distance. To prevent any single column from having too much of an impact on the distance, we can normalize all of the columns to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "Normalizing the values in each column to the standard normal distribution (mean of 0, standard deviation of 1) preserves the distribution of the values in each column while aligning the scales. To normalize the values in a column to the standard normal distribution, you need to:\n",
    "\n",
    "from each value, subtract the mean of the column\n",
    "divide each value by the standard deviation of the column\n",
    "Here's the mathematical formula describing the transformation that needs to be applied for all values in a column:\n",
    "\n",
    "x=(x-mu)/sigma\n",
    "\n",
    "where  is a value in a specific column,  is the mean of all the values in the column, and  is the standard deviation of all the values in the column. Here's what the corresponding code, using pandas, looks like:\n",
    "\n",
    "So, lets normalize the columns in dc_listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      accommodates  bedrooms  bathrooms      beds  price  minimum_nights  \\\n",
      "574      -0.596544 -0.249467  -0.439151 -0.546858  125.0       -0.341375   \n",
      "1593     -0.596544 -0.249467   0.412923 -0.546858   85.0       -0.341375   \n",
      "3091     -1.095499 -0.249467  -1.291226 -0.546858   50.0       -0.341375   \n",
      "\n",
      "      maximum_nights  number_of_reviews  \n",
      "574        -0.016604           4.579650  \n",
      "1593       -0.016603           1.159275  \n",
      "3091       -0.016573          -0.482505  \n"
     ]
    }
   ],
   "source": [
    "normalized_listings = (dc_listings - dc_listings.mean())/(dc_listings.std())\n",
    "normalized_listings['price'] = dc_listings['price']\n",
    "print(normalized_listings.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.272543124668404\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "first_listing = normalized_listings.iloc[0][['accommodates', 'bathrooms']]\n",
    "fifth_listing = normalized_listings.iloc[4][['accommodates', 'bathrooms']]\n",
    "first_fifth_distance = distance.euclidean(first_listing, fifth_listing)\n",
    "print(first_fifth_distance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the distance.euclidean() function from scipy.spatial, which takes in 2 vectors as the parameters and calculates the Euclidean distance between them. The euclidean() function expects:\n",
    "\n",
    "-Both of the vectors to be represented using a list-like object (Python list, NumPy array, or pandas Series)\n",
    "-Both of the vectors must be 1-dimensional and have the same number of elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13474.403367463026\n",
      "116.07929775572829\n"
     ]
    }
   ],
   "source": [
    "#Create an instance of the KNeighborsRegressor class with the following parameters\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "#n_neighbors: 5\n",
    "#algorithm: brute\n",
    "#Use the fit method to specify the data we want the k-nearest neighbor model to use. Use the following parameters:\n",
    "train_df=normalized_listings.iloc[0:2792]\n",
    "test_df=normalized_listings.iloc[2792:]\n",
    "train_columns=['accommodates','bathrooms','bedrooms','number_of_reviews']\n",
    "\n",
    "#Instantiate ML model\n",
    "knn=KNeighborsRegressor(n_neighbors=5,algorithm='brute')\n",
    "\n",
    "#Now, lets fit model to the data\n",
    "knn.fit(train_df[train_columns], train_df['price'])\n",
    "\n",
    "#use model to ake predictions\n",
    "predictions=knn.predict(test_df[train_columns])\n",
    "test_df\n",
    "\n",
    "#Clacluate MSE using Scikit Learn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "two_features_mse = mean_squared_error(test_df['price'], predictions)\n",
    "two_features_rmse = two_features_mse ** (1/2)\n",
    "print(two_features_mse)\n",
    "print(two_features_rmse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we proceed and increase the features, we observed lower mse and rmse\n",
    "Another way to only select dependent feature is to:\n",
    "features = train_df.columns.tolist()\n",
    "features.remove('price')\n",
    "It will automatically select the independent features and leave the dependent variable 'price'\n",
    "\n",
    "Interestingly enough, the RMSE value actually increased to 125.1 when we used all of the features available to us. This means that selecting the right features is important and that using more features doesn't automatically improve prediction accuracy. We should re-phrase the lever we mentioned earlier from:\n",
    "\n",
    "1)Increase the number of attributes the model uses to calculate similarity when ranking the closest neighbors\n",
    "to:\n",
    "\n",
    "2)Select the relevant attributes the model uses to calculate similarity when ranking the closest neighbors\n",
    "The process of selecting features to use in a model is known as feature selection.\n",
    "\n",
    "In this mission, we prepared the data to be able to use more features, trained a few models using multiple features, and evaluated the different performance tradeoffs. We explored how using more features doesn't always improve the accuracy of a k-nearest neighbors model. In the next mission, we'll explore another knob for tuning k-nearest neighbor models - the k value\n",
    "\n",
    "Lets move into Hyper Optimization to select the best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('dc_airbnb_train.csv')\n",
    "test_df = pd.read_csv('dc_airbnb_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26383.244596131968, 15152.84755403868, 14668.32890911389, 16079.745093856654, 14233.64313993174]\n"
     ]
    }
   ],
   "source": [
    "#Create a list containing the integer values 1, 2, 3, 4, and 5, in that order, and assign to hyper_params.\n",
    "features = ['accommodates', 'bedrooms', 'bathrooms', 'number_of_reviews']\n",
    "hyper_params = [1, 2, 3, 4, 5]\n",
    "\n",
    "#Create an empty list and assign to mse_values.\n",
    "mse_values = list()\n",
    "\n",
    "#Use a for loop to iterate over hyper_params and in each iteration:\n",
    "#Instantiate a KNeighborsRegressor object with the following parameters:\n",
    "#n_neighbors: the current value for the iterator variable,\n",
    "#algorithm: brute\n",
    "#Fit the instantiated k-nearest neighbors model to the following columns from train_df:\n",
    "#accommodates\n",
    "#bedrooms\n",
    "#bathrooms\n",
    "#number_of_reviews\n",
    "#Use the trained model to make predictions on the same columns from test_df and assign to predictions.\n",
    "#Use the mean_squared_error function to calculate the MSE value between predictions and the price column from test_df.\n",
    "#Append the MSE value to mse_values.\n",
    "#Display mse_values using the print() function.\n",
    "\n",
    "for hp in hyper_params:\n",
    "    knn = KNeighborsRegressor(n_neighbors=hp, algorithm='brute')\n",
    "    knn.fit(train_df[features], train_df['price'])\n",
    "    predictions = knn.predict(test_df[features])\n",
    "    mse = mean_squared_error(test_df['price'], predictions)\n",
    "    mse_values.append(mse)\n",
    "    \n",
    "print(mse_values)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our dataset is small and scikit-learn has been developed with performance in mind, the code ran quickly. As we increased the k value from 1 to 5, the MSE value fell from approximately 26364 to approximately 14090:\n",
    "\n",
    "k\tMSE\n",
    "1\t26364.928327645051\n",
    "2\t15100.522468714449\n",
    "3\t14579.597901655923\n",
    "4\t16212.300767918088\n",
    "5\t14090.011649601822\n",
    "\n",
    "Lets have more experiment by expanding a grid search all the way to a k value of 20. While 20 may seem like an arbitrary ending point for our grid search, we can always expand the values we try if we're unconvinced that the lowest MSE value is associated with one of the hyperparamter values we tried so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26383.244596131968, 15152.84755403868, 14668.32890911389, 16079.745093856654, 14233.64313993174, 13781.359025407659, 13932.903995727982, 14015.416293373151, 14040.643997808958, 14214.452400455062, 14323.282373847065, 14588.854711793701, 14576.310270546817, 14573.683493533936, 14642.491240045507, 14687.59449214306, 14719.012451236267, 14725.675413278275, 14780.722616672812, 14755.384158134244]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x9b2b7e66a0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "features = ['accommodates', 'bedrooms', 'bathrooms', 'number_of_reviews']\n",
    "hyper_params = [x for x in range(1, 21)]\n",
    "mse_values = list()\n",
    "\n",
    "for hp in hyper_params:\n",
    "    knn = KNeighborsRegressor(n_neighbors=hp, algorithm='brute')\n",
    "    knn.fit(train_df[features], train_df['price'])\n",
    "    predictions = knn.predict(test_df[features])\n",
    "    mse = mean_squared_error(test_df['price'], predictions)\n",
    "    mse_values.append(mse)\n",
    "print(mse_values)\n",
    "plt.scatter(hyper_params, mse_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we increased the k value from 1 to 6, the MSE value decreased from approximately 26364 to approximately 13657. However, as we increased the k value from 7 to 20, the MSE value didn't decrease further but instead hovered between approximately 14015 and 14755. This means that the optimal k value is 6, since it resulted in the lowest MSE value.\n",
    "\n",
    "From the scatter plot, you can tell that the lowest MSE value was achieved at the k value of 6. As we increased k past 6, the MSE actually increased and hovered but never decreased below 13781 (the approximate MSE value when k was 6)\n",
    "This pattern is something you'll notice while performing grid search across other models as well. As you increase k at first, the error rate decreases until a certain point, but then rebounds and increases again. Let's confirm this behavior visually using a scatter plot.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Now, lets put the k-nearest models to all the columns except for the price column\n",
    "hyper_params = [x for x in range(1,21)]\n",
    "mse_values = list()\n",
    "features = train_df.columns.tolist()\n",
    "features.remove('price')\n",
    "\n",
    "for hp in hyper_params:\n",
    "    knn = KNeighborsRegressor(n_neighbors=hp, algorithm='brute')\n",
    "    knn.fit(train_df[features], train_df['price'])\n",
    "    predictions = knn.predict(test_df[features])\n",
    "    mse = mean_squared_error(test_df['price'], predictions)\n",
    "    mse_values.append(mse)\n",
    "\n",
    "plt.scatter(hyper_params, mse_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While using only the accommodates and bathrooms columns:\n",
    "\n",
    "Train a model for each k value between 1 and 20 using the training data.\n",
    "Use each model to make predictions on the test set (using just the accommodates and bathrooms columns).\n",
    "Calculate each model's MSE value by comparing each set of predictions to the true price values.\n",
    "Find the k value that obtained the lowest MSE value.\n",
    "Create a dictionary named two_hyp_mse that contains 1 key-value pair:\n",
    "key: k value that resulted in lowest MSE value.\n",
    "value: corresponding MSE value.\n",
    "Repeat this process while using only the accommodates, bathrooms, and bedrooms columns:\n",
    "\n",
    "Create a dictionary named three_hyp_mse that contains 1 key-value pair:\n",
    "key: k value that resulted in lowest MSE value.\n",
    "value: corresponding MSE value.\n",
    "Display both two_hyp_mse and three_hyp_mse using the print() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{5: 14875.319590443689}\n",
      "{5: 13494.16850967008}\n"
     ]
    }
   ],
   "source": [
    "two_features = ['accommodates', 'bathrooms']\n",
    "three_features = ['accommodates', 'bathrooms', 'bedrooms']\n",
    "hyper_params = [x for x in range(1,21)]\n",
    "# Append the first model's MSE values to this list.\n",
    "two_mse_values = list()\n",
    "# Append the second model's MSE values to this list.\n",
    "three_mse_values = list()\n",
    "two_hyp_mse = dict()\n",
    "three_hyp_mse = dict()\n",
    "for hp in hyper_params:\n",
    "    knn = KNeighborsRegressor(n_neighbors=hp, algorithm='brute')\n",
    "    knn.fit(train_df[two_features], train_df['price'])\n",
    "    predictions = knn.predict(test_df[two_features])\n",
    "    mse = mean_squared_error(test_df['price'], predictions)\n",
    "    two_mse_values.append(mse)\n",
    "\n",
    "two_lowest_mse = two_mse_values[0]\n",
    "two_lowest_k = 1\n",
    "\n",
    "for k,mse in enumerate(two_mse_values):\n",
    "    if mse < two_lowest_mse:\n",
    "        two_lowest_mse = mse\n",
    "        two_lowest_k = k + 1\n",
    "    \n",
    "for hp in hyper_params:\n",
    "    knn = KNeighborsRegressor(n_neighbors=hp, algorithm='brute')\n",
    "    knn.fit(train_df[three_features], train_df['price'])\n",
    "    predictions = knn.predict(test_df[three_features])\n",
    "    mse = mean_squared_error(test_df['price'], predictions)\n",
    "    three_mse_values.append(mse)\n",
    "    \n",
    "three_lowest_mse = three_mse_values[0]\n",
    "three_lowest_k = 1\n",
    "\n",
    "for k,mse in enumerate(three_mse_values):\n",
    "    if mse < three_lowest_mse:\n",
    "        three_lowest_mse = mse\n",
    "        three_lowest_k = k + 1\n",
    "\n",
    "two_hyp_mse[two_lowest_k] = two_lowest_mse\n",
    "three_hyp_mse[three_lowest_k] = three_lowest_mse\n",
    "\n",
    "print(two_hyp_mse)\n",
    "print(three_hyp_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model, which used the accommodates and bathrooms columns, was able to achieve an MSE value of approximately 14790. The second model, which added the bedrooms column, was able to achieve an MSE value of approximately 13522.9, which is even lower than the lowest MSE value we achieved using the best model from the last mission (which used the accommodates, bedrooms, bathrooms, and number_of_reviews columns). Hopefully this demonstrates that using just one lever to find the best model isn't enough and you really want to use both levers in conjunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Cross validation \n",
    "we'll focus on the holdout validation technique, which involves:\n",
    "\n",
    "splitting the full dataset into 2 partitions:\n",
    "a training set\n",
    "a test set\n",
    "training the model on the training set,\n",
    "using the trained model to predict labels on the test set,\n",
    "computing an error metric to understand the model's effectiveness,\n",
    "switch the training and test sets and repeat,\n",
    "average the errors.\n",
    "In holdout validation, we usually use a 50/50 split instead of the 75/25 split from train/test validation. This way, we remove number of observations as a potential source of variation in our model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118.9972283485966 142.82709375697377 130.91216105278517\n"
     ]
    }
   ],
   "source": [
    "dc_listings = pd.read_csv(\"dc_airbnb.csv\")\n",
    "stripped_commas = dc_listings['price'].str.replace(',', '')\n",
    "stripped_dollars = stripped_commas.str.replace('$', '')\n",
    "dc_listings['price'] = stripped_dollars.astype('float')\n",
    "shuffled_index = np.random.permutation(dc_listings.index)\n",
    "dc_listings = dc_listings.reindex(shuffled_index)\n",
    "\n",
    "split_one = dc_listings.iloc[0:1862].copy()\n",
    "split_two = dc_listings.iloc[1862:].copy()\n",
    "\n",
    "train_one = split_one\n",
    "test_one = split_two\n",
    "train_two = split_two\n",
    "test_two = split_one\n",
    "# First half\n",
    "model = KNeighborsRegressor()\n",
    "model.fit(train_one[[\"accommodates\"]], train_one[\"price\"])\n",
    "test_one[\"predicted_price\"] = model.predict(test_one[[\"accommodates\"]])\n",
    "iteration_one_rmse = mean_squared_error(test_one[\"price\"], test_one[\"predicted_price\"])**(1/2)\n",
    "\n",
    "# Second half\n",
    "model.fit(train_two[[\"accommodates\"]], train_two[\"price\"])\n",
    "test_two[\"predicted_price\"] = model.predict(test_two[[\"accommodates\"]])\n",
    "iteration_two_rmse = mean_squared_error(test_two[\"price\"], test_two[\"predicted_price\"])**(1/2)\n",
    "\n",
    "avg_rmse = np.mean([iteration_two_rmse, iteration_one_rmse])\n",
    "\n",
    "print(iteration_one_rmse, iteration_two_rmse, avg_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If we average the two RMSE values from the last step, we get an RMSE value of approximately 130.91. Holdout validation is actually a specific example of a larger class of validation techniques called k-fold cross-validation. While holdout validation is better than train/test validation because the model isn't repeatedly biased towards a specific subset of the data, both models that are trained only use half the available data. K-fold cross validation, on the other hand, takes advantage of a larger proportion of the data during training while still rotating through different subsets of the data to avoid the issues of train/test validation.\n",
    "\n",
    "#kcross validation\n",
    "Here's the algorithm from k-fold cross validation:\n",
    "\n",
    "-splitting the full dataset into k equal length partitions.\n",
    "-selecting k-1 partitions as the training set and\n",
    "-selecting the remaining partition as the test set\n",
    "-training the model on the training set.\n",
    "-using the trained model to predict labels on the test fold.\n",
    "-computing the test fold's error metric.\n",
    "-repeating all of the above steps k-1 times, until each partition has been used as the test set for an iteration.\n",
    "calculating the mean of the k error values.\n",
    "-Holdout validation is essentially a version of k-fold cross validation when k is equal to 2. Generally, 5 or 10 folds is used for k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0    745\n",
      "2.0    745\n",
      "1.0    745\n",
      "4.0    744\n",
      "3.0    744\n",
      "Name: fold, dtype: int64\n",
      "\n",
      " Num of missing values:  0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dc_listings.loc[dc_listings.index[0:745], \"fold\"] = 1\n",
    "dc_listings.loc[dc_listings.index[745:1490], \"fold\"] = 2\n",
    "dc_listings.loc[dc_listings.index[1490:2234], \"fold\"] = 3\n",
    "dc_listings.loc[dc_listings.index[2234:2978], \"fold\"] = 4\n",
    "dc_listings.loc[dc_listings.index[2978:3723], \"fold\"] = 5\n",
    "print(dc_listings['fold'].value_counts())\n",
    "print(\"\\n Num of missing values: \", dc_listings['fold'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Train a k-nearest neighbors model using the accommodates column as the sole feature from folds 2 to 5 as the training set.\n",
    "-Use the model to make predictions on the test set (accommodates column from fold 1) and assign the predicted labels to labels.\n",
    "-Calculate the RMSE value by comparing the price column with the predicted labels.\n",
    "-Assign the RMSE value to iteration_one_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "model = KNeighborsRegressor()\n",
    "train_iteration_one = dc_listings[dc_listings[\"fold\"] != 1]\n",
    "test_iteration_one = dc_listings[dc_listings[\"fold\"] == 1].copy()\n",
    "model.fit(train_iteration_one[[\"accommodates\"]], train_iteration_one[\"price\"])\n",
    "\n",
    "# Predicting\n",
    "labels = model.predict(test_iteration_one[[\"accommodates\"]])\n",
    "test_iteration_one[\"predicted_price\"] = labels\n",
    "iteration_one_mse = mean_squared_error(test_iteration_one[\"price\"], test_iteration_one[\"predicted_price\"])\n",
    "iteration_one_rmse = iteration_one_mse ** (1/2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function named train_and_validate that takes in a dataframe as the first parameter (df) and a list of fold values (1 to 5 in our case) as the second parameter (folds). This function should:\n",
    "\n",
    "Train n models (where n is number of folds) and perform k-fold cross validation (using n folds). Use the default k value for the KNeighborsRegressor class.\n",
    "Return a list of RMSE values, where the first element is the RMSE for when fold 1 was the test set, the second element is the RMSE for when fold 2 was the test set, and so on.\n",
    "Use the train_and_validate function to return the list of RMSE values for the dc_listings Dataframe and assign to rmses.\n",
    "\n",
    "Calculate the mean of these values and assign to avg_rmse.\n",
    "Display both rmses and avg_rmse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[134.48877940272527, 121.4580456744649, 147.52190305754746, 96.314530944026, 145.92810402257734]\n",
      "129.1422726202682\n"
     ]
    }
   ],
   "source": [
    "# Use np.mean to calculate the mean.\n",
    "import numpy as np\n",
    "fold_ids = [1,2,3,4,5]\n",
    "def train_and_validate(df, folds):\n",
    "    fold_rmses = []\n",
    "    for fold in folds:\n",
    "        # Train\n",
    "        model = KNeighborsRegressor()\n",
    "        train = df[df[\"fold\"] != fold]\n",
    "        test = df[df[\"fold\"] == fold].copy()\n",
    "        model.fit(train[[\"accommodates\"]], train[\"price\"])\n",
    "        # Predict\n",
    "        labels = model.predict(test[[\"accommodates\"]])\n",
    "        test[\"predicted_price\"] = labels\n",
    "        mse = mean_squared_error(test[\"price\"], test[\"predicted_price\"])\n",
    "        rmse = mse**(1/2)\n",
    "        fold_rmses.append(rmse)\n",
    "    return(fold_rmses)\n",
    "\n",
    "rmses = train_and_validate(dc_listings, fold_ids)\n",
    "print(rmses)\n",
    "avg_rmse = np.mean(rmses)\n",
    "print(avg_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using k-cross validation through scikit learn\n",
    "In machine learning, we're interested in building a good model and accurately understanding how well it will perform. To build a better k-nearest neighbors model, we can change the features it uses or tweak the number of neighbors (a hyperparameter). To accurately understand a model's performance, we can perform k-fold cross validation and select the proper number of folds. We've learned how scikit-learn makes it easy for us to quickly experiment with these different knobs when it comes to building a better model. Let's now dive into how we can use scikit-learn to handle cross-validation as well.\n",
    "\n",
    "First, we instantiate an instance of the KFold class from sklearn.model_selection:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits, shuffle=False, random_state=None)\n",
    "where:\n",
    "\n",
    "n_splits is the number of folds you want to use,\n",
    "shuffle is used to toggle shuffling of the ordering of the observations in the dataset,\n",
    "random_state is used to specify the random seed value if shuffle is set to True.\n",
    "You'll notice here that no parameters depend on the data set at all. This is because the KFold class returns an iterator object which we use in conjunction with the cross_val_score() function, also from sklearn.model_selection. Together, these 2 functions allow us to compactly train and test using k-fold cross validation:\n",
    "\n",
    "Here are the relevant parameters for the cross_val_score function:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(estimator, X, Y, scoring=None, cv=None)\n",
    "where:\n",
    "\n",
    "estimator is a sklearn model that implements the fit method (e.g. instance of KNeighborsRegressor),\n",
    "X is the list or 2D array containing the features you want to train on,\n",
    "y is a list containing the values you want to predict (target column),\n",
    "scoring is a string describing the scoring criteria (list of accepted values here).\n",
    "cv describes the number of folds. Here are some examples of accepted values:\n",
    "an instance of the KFold class,\n",
    "an integer representing the number of folds.\n",
    "Depending on the scoring criteria you specify, a single total value is returned for each fold. Here's the general workflow for performing k-fold cross-validation using the classes we just described:\n",
    "\n",
    "instantiate the scikit-learn model class you want to fit,\n",
    "instantiate the KFold class and using the parameters to specify the k-fold cross-validation attributes you want,\n",
    "use the cross_val_score() function to return the scoring metric you're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[149.55342652 157.37125924 129.69069699 120.28923029 134.58266836]\n",
      "138.29745628067826\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "kf = KFold(5, shuffle=True, random_state=1)\n",
    "model = KNeighborsRegressor()\n",
    "mses = cross_val_score(model, dc_listings[[\"accommodates\"]], dc_listings[\"price\"], scoring=\"neg_mean_squared_error\", cv=kf)\n",
    "rmses = np.sqrt(np.absolute(mses))\n",
    "avg_rmse = np.mean(rmses)\n",
    "\n",
    "print(rmses)\n",
    "print(avg_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
